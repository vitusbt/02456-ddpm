{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Denoising Diffusion Probabilistic Models\n",
    "*Caroline Wrist-Jensen (s194349), Christian Schaumburg Jakobsen (s194307), Niklas Kristian Jensen (s194340) and Vitus BÃ¸dker Thomsen (s194331)*\n",
    "\n",
    "This notebook contains the code for our implementation of Denoising Diffusion Probabilistic Models (DDPM), as well as our addition of Classifier-Free Guidance (CFG). It contains the same code as `ddpm.py` and `ddpm_cfg.py`, but simply in notebook format.\n",
    "\n",
    "Note: the DDPM implementation is based on a tutorial by DeepFindr:\n",
    "* Original implementation: https://colab.research.google.com/drive/1sjy9odlSSy0RBVgMTgP7s99NXsqglsUL\n",
    "* Video tutorial: https://www.youtube.com/watch?v=a4Yfz2FxXiY\n",
    "\n",
    "The code has been modified by us to suit our needs, and the addition of Classifier-Free Guidance is also done by us.\n",
    "\n",
    "Note: the code is intended to be run on GPU - it is extremely slow on CPU!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.nn.init as init\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.nn.parameter import Parameter\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam, lr_scheduler\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "#!pip install labml-nn\n",
    "from labml_nn.diffusion.ddpm.unet import UNet\n",
    "\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Hyperparameters\n",
    "BATCH_SIZE = 128\n",
    "T = 1000\n",
    "UNET_N_CHANNELS = 64 # must be a multiple of 32\n",
    "LR = 0.00002\n",
    "LR_SCHEDULER_STEP_SIZE = 20\n",
    "LR_SCHEDULER_GAMMA = 0.5\n",
    "N_EPOCHS = 60\n",
    "SAMPLE_BATCH_SIZE = 256\n",
    "N_SAMPLE_BATCHES = 50\n",
    "DATASET = 'MNIST'\n",
    "run_name = 'mnist_final3'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "if DATASET == 'MNIST':\n",
    "    IMG_CHANNELS = 1\n",
    "    IMG_SIZE=32\n",
    "    dataset = utils.load_MNIST()\n",
    "elif DATASET == 'CIFAR10':\n",
    "    IMG_CHANNELS = 3\n",
    "    IMG_SIZE=32\n",
    "    dataset = utils.load_CIFAR10()\n",
    "else:\n",
    "    raise ValueError('Dataset must be MNIST or CIFAR10')\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define some utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_beta_schedule(timesteps, start=0.0001, end=0.02):\n",
    "    return torch.linspace(start, end, timesteps)\n",
    "\n",
    "# Define beta schedule\n",
    "betas = linear_beta_schedule(timesteps=T).to(device)                            # Variance of q(x_t|x_{t-1})\n",
    "\n",
    "# Pre-calculate different terms for closed form\n",
    "alphas = 1. - betas                                                             # 1 - beta\n",
    "alphas_cumprod = torch.cumprod(alphas, axis=0)                                  # alpha_bar\n",
    "alphas_cumprod_prev = F.pad(alphas_cumprod[:-1], (1, 0), value=1.0)             \n",
    "sqrt_recip_alphas = torch.sqrt(1.0 / alphas)                                    # \n",
    "sqrt_alphas_cumprod = torch.sqrt(alphas_cumprod)                                # mean of q(x_t|x_0)\n",
    "sqrt_one_minus_alphas_cumprod = torch.sqrt(1. - alphas_cumprod)                 # variance of q(x_t|x_0)\n",
    "posterior_variance = betas * (1. - alphas_cumprod_prev) / (1. - alphas_cumprod) # posterior variance q(x_{t-1}|x_t,x_0)\n",
    "\n",
    "def get_index_from_list(vals, t, x_shape):\n",
    "    \"\"\"\n",
    "    Returns a specific index t of a passed list of values vals\n",
    "    while considering the batch dimension.\n",
    "    \"\"\"\n",
    "    batch_size = t.shape[0]\n",
    "    out = vals.gather(-1, t)\n",
    "    return out.reshape(batch_size, *((1,) * (len(x_shape) - 1))).to(t.device)\n",
    "\n",
    "def forward_diffusion_sample(x_0, t, device=device):\n",
    "    \"\"\"\n",
    "    Takes an image and a timestep as input and\n",
    "    returns the noisy version of it\n",
    "    \"\"\"\n",
    "    noise = torch.randn_like(x_0)\n",
    "    sqrt_alphas_cumprod_t = get_index_from_list(sqrt_alphas_cumprod, t, x_0.shape)\n",
    "    sqrt_one_minus_alphas_cumprod_t = get_index_from_list(\n",
    "        sqrt_one_minus_alphas_cumprod, t, x_0.shape\n",
    "    )\n",
    "    # mean + variance\n",
    "    return sqrt_alphas_cumprod_t.to(device) * x_0.to(device) \\\n",
    "    + sqrt_one_minus_alphas_cumprod_t.to(device) * noise.to(device), noise.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DDPM (unconditional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UNet(image_channels=IMG_CHANNELS, n_channels=UNET_N_CHANNELS)\n",
    "model.to(device)\n",
    "\n",
    "print(\"Num params: \", sum(p.numel() for p in model.parameters()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss(model, x_0, t):\n",
    "    x_noisy, noise = forward_diffusion_sample(x_0, t, device)\n",
    "    noise_pred = model(x_noisy, t)\n",
    "    return F.mse_loss(noise, noise_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function for taking a single step of the reverse process on a batch of images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def sample_timestep(x, t, noise=None):\n",
    "    \"\"\"\n",
    "    Calls the model to predict the noise in the image and returns\n",
    "    the denoised image.\n",
    "    Applies noise to this image, if we are not in the last step yet.\n",
    "    \"\"\"\n",
    "    betas_t = get_index_from_list(betas, t, x.shape)\n",
    "    sqrt_one_minus_alphas_cumprod_t = get_index_from_list(\n",
    "        sqrt_one_minus_alphas_cumprod, t, x.shape\n",
    "    )\n",
    "    sqrt_recip_alphas_t = get_index_from_list(sqrt_recip_alphas, t, x.shape)\n",
    "\n",
    "    # Call model (current image - noise prediction)\n",
    "    model_mean = sqrt_recip_alphas_t * (\n",
    "        x - betas_t * model(x, t) / sqrt_one_minus_alphas_cumprod_t\n",
    "    )\n",
    "    posterior_variance_t = get_index_from_list(posterior_variance, t, x.shape)\n",
    "\n",
    "    if t[0] == 0:\n",
    "        return model_mean\n",
    "    else:\n",
    "        if noise is None:\n",
    "            noise = torch.randn_like(x)\n",
    "        return model_mean + torch.sqrt(posterior_variance_t) * noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define optimizer and run the main training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device)\n",
    "optimizer = Adam(model.parameters(), lr=LR)\n",
    "scheduler = lr_scheduler.StepLR(optimizer, LR_SCHEDULER_STEP_SIZE, gamma=LR_SCHEDULER_GAMMA)\n",
    "\n",
    "epoch_loss = np.zeros(N_EPOCHS)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(N_EPOCHS):\n",
    "    for step, batch in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        t = torch.randint(0, T, (BATCH_SIZE,), device=device).long()\n",
    "        loss = get_loss(model, batch[0], t)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss[epoch] += loss.item()\n",
    "\n",
    "        if np.isnan(loss.item()):\n",
    "            raise Exception('Loss is NaN')\n",
    "\n",
    "        if step % 50 == 0:\n",
    "            print(f\"Epoch {epoch} | step {step:03d} Loss: {loss.item()} \")\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    epoch_loss[epoch] /= (step+1)\n",
    "    print(f\"Epoch {epoch} | Final Loss: {epoch_loss[epoch]} \")\n",
    "\n",
    "\n",
    "# Save the loss after each epoch\n",
    "np.save('epoch_loss_'+run_name+'.npy', epoch_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define functions for sampling a batch of images (running the entire reverse process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def sample_batch():\n",
    "    \"\"\"\n",
    "    Sample a batch of SAMPLE_BATCH_SIZE images\n",
    "    \"\"\"\n",
    "    img_size = IMG_SIZE\n",
    "    img = torch.randn((SAMPLE_BATCH_SIZE, IMG_CHANNELS, img_size, img_size), device=device)\n",
    "\n",
    "    for i in range(0,T)[::-1]:\n",
    "        t = torch.full((SAMPLE_BATCH_SIZE,), i, device=device, dtype=torch.long)\n",
    "        noise = torch.randn_like(img)\n",
    "        img = sample_timestep(img, t, noise)\n",
    "\n",
    "    # Clamp the image to the range [-1,1]\n",
    "    img = torch.clamp(img, -1.0, 1.0)\n",
    "\n",
    "    return img\n",
    "\n",
    "@torch.no_grad()\n",
    "def sample_batch_steps(step=100):\n",
    "    \"\"\"\n",
    "    Sample a batch of SAMPLE_BATCH_SIZE and save the image every 100 steps\n",
    "    for visualizing the sampling process\n",
    "    \"\"\"\n",
    "    img_size = IMG_SIZE\n",
    "    img = torch.randn((SAMPLE_BATCH_SIZE, IMG_CHANNELS, img_size, img_size), device=device)\n",
    "\n",
    "    img_steps = [img]\n",
    "\n",
    "    for i in range(0,T)[::-1]:\n",
    "        t = torch.full((SAMPLE_BATCH_SIZE,), i, device=device, dtype=torch.long)\n",
    "        noise = torch.randn_like(img)\n",
    "        img = sample_timestep(img, t, noise)\n",
    "        if i % step == 0:\n",
    "            img_steps.append(img)\n",
    "\n",
    "    img_steps = torch.stack(img_steps, dim=1)\n",
    "    img_steps = torch.clamp(img_steps, -1.0, 1.0)\n",
    "\n",
    "    return img_steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample a bunch of images\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "if not os.path.exists(f'samples_{run_name}'):\n",
    "    os.makedirs(f'samples_{run_name}/all_samples')\n",
    "    os.makedirs(f'samples_{run_name}/steps')\n",
    "    os.makedirs(f'samples_{run_name}/grids')\n",
    "\n",
    "# sample all batches\n",
    "for batch in range(N_SAMPLE_BATCHES):\n",
    "    print(f'sampling batch {batch}')\n",
    "    imgs = sample_batch()\n",
    "    imgs = (imgs + 1) * 0.5 # Transform to the range [0,1]\n",
    "    for i, img in enumerate(imgs):\n",
    "        save_image(img, f'samples_{run_name}/all_samples/batch{batch:03d}_img{i:03d}.png')\n",
    "    img_grid = make_grid(imgs, nrow=int(np.sqrt(SAMPLE_BATCH_SIZE)))\n",
    "    save_image(img_grid, f'samples_{run_name}/grids/batch{batch:03d}_grid.png')\n",
    "\n",
    "# Sample a single batch where we show the steps\n",
    "print('sampling batch with steps')\n",
    "imgs_steps = sample_batch_steps()\n",
    "imgs_steps = (imgs_steps + 1) * 0.5 # Transform to the range [0,1]\n",
    "for i, img in enumerate(imgs_steps):\n",
    "    img_step_sequence = make_grid(img, 11)\n",
    "    save_image(img_step_sequence, f'samples_{run_name}/steps/steps{i:03d}.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier-Free Guidance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reset the hyperparameters as needed and define new hyperparameter P_UNCOND"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Hyperparameters\n",
    "BATCH_SIZE = 128\n",
    "T = 1000\n",
    "UNET_N_CHANNELS = 64 # must be a multiple of 32\n",
    "LR = 0.00002\n",
    "LR_SCHEDULER_STEP_SIZE = 30\n",
    "LR_SCHEDULER_GAMMA = 0.5\n",
    "N_EPOCHS = 90\n",
    "SAMPLE_BATCH_SIZE = 256\n",
    "N_SAMPLE_BATCHES = 50\n",
    "DATASET = 'CIFAR10'\n",
    "run_name = 'cifar_cfg_final1'\n",
    "\n",
    "# New hyperparameter controlling the class dropout rate\n",
    "P_UNCOND = 0.2\n",
    "\n",
    "# Load dataset\n",
    "if DATASET == 'MNIST':\n",
    "    IMG_CHANNELS = 1\n",
    "    IMG_SIZE=32\n",
    "    dataset = utils.load_MNIST()\n",
    "elif DATASET == 'CIFAR10':\n",
    "    IMG_CHANNELS = 3\n",
    "    IMG_SIZE=32\n",
    "    dataset = utils.load_CIFAR10()\n",
    "else:\n",
    "    raise ValueError('Dataset must be MNIST or CIFAR10')\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unet_cfg import UNetCFG\n",
    "\n",
    "model = UNetCFG(image_channels=IMG_CHANNELS, n_channels=UNET_N_CHANNELS)\n",
    "model.to(device)\n",
    "\n",
    "print(\"Num params: \", sum(p.numel() for p in model.parameters()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss(model, x_0, t, c):\n",
    "    x_noisy, noise = forward_diffusion_sample(x_0, t, device)\n",
    "    noise_pred = model(x_noisy, t, c)\n",
    "    return F.mse_loss(noise, noise_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function for taking a single step of the reverse process on a batch of images, now taking both the conditional and unconditional model into account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def sample_timestep_cfg(x, t, c, noise=None, w=0.5):\n",
    "    \"\"\"\n",
    "    Calls the model to predict the noise in the image and returns\n",
    "    the denoised image.\n",
    "    Applies noise to this image, if we are not in the last step yet.\n",
    "    \"\"\"\n",
    "    betas_t = get_index_from_list(betas, t, x.shape)\n",
    "    sqrt_one_minus_alphas_cumprod_t = get_index_from_list(\n",
    "        sqrt_one_minus_alphas_cumprod, t, x.shape\n",
    "    )\n",
    "    sqrt_recip_alphas_t = get_index_from_list(sqrt_recip_alphas, t, x.shape)\n",
    "\n",
    "    c_uncond = torch.full(t.size(), 10, device=device, dtype=torch.long)\n",
    "\n",
    "    pred = model(x, t, c)\n",
    "    if w != 0:\n",
    "        pred_uncond = model(x, t, c_uncond)\n",
    "        epsilon = (1+w)*pred - w*pred_uncond\n",
    "    else:\n",
    "        epsilon = pred\n",
    "\n",
    "    # Call model (current image - noise prediction)\n",
    "    model_mean = sqrt_recip_alphas_t * (\n",
    "        x - betas_t * epsilon / sqrt_one_minus_alphas_cumprod_t\n",
    "    )\n",
    "    posterior_variance_t = get_index_from_list(posterior_variance, t, x.shape)\n",
    "\n",
    "    if t[0] == 0:\n",
    "        return model_mean\n",
    "    else:\n",
    "        if noise is None:\n",
    "            noise = torch.randn_like(x)\n",
    "        return model_mean + torch.sqrt(posterior_variance_t) * noise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define optimizer and run the main training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device)\n",
    "optimizer = Adam(model.parameters(), lr=LR)\n",
    "scheduler = lr_scheduler.StepLR(optimizer, LR_SCHEDULER_STEP_SIZE, gamma=LR_SCHEDULER_GAMMA)\n",
    "\n",
    "epoch_loss = np.zeros(N_EPOCHS)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(N_EPOCHS):\n",
    "    for step, batch in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        t = torch.randint(0, T, (BATCH_SIZE,), device=device).long()\n",
    "        imgs = batch[0].to(device)\n",
    "        classes = batch[1]\n",
    "\n",
    "        # Randomly dropout the classes\n",
    "        u = torch.rand(classes.size())\n",
    "        classes[u < P_UNCOND] = 10\n",
    "        classes = classes.to(device)\n",
    "\n",
    "        loss = get_loss(model, imgs, t, classes)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss[epoch] += loss.item()\n",
    "\n",
    "        if np.isnan(loss.item()):\n",
    "            raise Exception('Loss is NaN')\n",
    "\n",
    "        if step % 50 == 0:\n",
    "            print(f\"Epoch {epoch} | step {step:03d} Loss: {loss.item()} \")\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    epoch_loss[epoch] /= (step+1)\n",
    "    print(f\"Epoch {epoch} | Final Loss: {epoch_loss[epoch]} \")\n",
    "\n",
    "# Save the loss after each epoch\n",
    "np.save('epoch_loss_'+run_name+'.npy', epoch_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define functions for sampling a batch of images (running the entire reverse process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample a batch using a given class c and a guidance weight w.\n",
    "# If c is None, sample 10 images from each class instead.\n",
    "@torch.no_grad()\n",
    "def sample_batch(c, w):\n",
    "    img_size = IMG_SIZE\n",
    "    bsize = 100 if c is None else SAMPLE_BATCH_SIZE\n",
    "    img = torch.randn((bsize, IMG_CHANNELS, img_size, img_size), device=device)\n",
    "\n",
    "    if c is None:\n",
    "        c = torch.repeat_interleave(torch.arange(10), 10).to(device)\n",
    "    else:\n",
    "        c = torch.full((bsize,), c, device=device, dtype=torch.long)\n",
    "\n",
    "    for i in range(0,T)[::-1]:\n",
    "        t = torch.full((bsize,), i, device=device, dtype=torch.long)\n",
    "        noise = torch.randn_like(img)\n",
    "        img = sample_timestep_cfg(img, t, c, noise, w=w)\n",
    "\n",
    "    # Clamp the image to the range [-1,1]\n",
    "    img = torch.clamp(img, -1.0, 1.0)\n",
    "\n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample a bunch of images\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "if not os.path.exists(f'samples_{run_name}'):\n",
    "    os.makedirs(f'samples_{run_name}/all_samples/w0')\n",
    "    os.makedirs(f'samples_{run_name}/all_samples/w1')\n",
    "    os.makedirs(f'samples_{run_name}/all_samples/w2')\n",
    "    os.makedirs(f'samples_{run_name}/grids/w0')\n",
    "    os.makedirs(f'samples_{run_name}/grids/w1')\n",
    "    os.makedirs(f'samples_{run_name}/grids/w2')\n",
    "    os.makedirs(f'samples_{run_name}/class_grids')\n",
    "\n",
    "# Samples\n",
    "for w_idx, w in enumerate([0, 0.5, 1]):\n",
    "    print(f'w={w}')\n",
    "    for batch in range(N_SAMPLE_BATCHES):\n",
    "        print(f'sampling batch {batch}')\n",
    "        imgs = sample_batch(c = batch % 10, w=w)\n",
    "        imgs = (imgs + 1) * 0.5 # Transform to the range [0,1]\n",
    "        for i, img in enumerate(imgs):\n",
    "            save_image(img, f'samples_{run_name}/all_samples/w{w_idx}/batch{batch:03d}_img{i:03d}.png')\n",
    "        img_grid = make_grid(imgs, nrow=int(np.sqrt(SAMPLE_BATCH_SIZE)))\n",
    "        print('saving image')\n",
    "        save_image(img_grid, f'samples_{run_name}/grids/w{w_idx}/batch{batch:03d}_grid.png')\n",
    "\n",
    "# Samples with class grid\n",
    "for w_idx, w in enumerate([0, 0.5, 1]):\n",
    "    print(f'sampling class grid w={w}')\n",
    "    imgs = sample_batch(c=None, w=w)\n",
    "    imgs = (imgs + 1) * 0.5 # Transform to the range [0,1]\n",
    "    img_grid = make_grid(imgs, nrow=10)\n",
    "    print('saving image')\n",
    "    save_image(img_grid, f'samples_{run_name}/class_grids/w{w_idx}.png')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
